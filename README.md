# ğŸ“Š Data Ingestion Tool

An end-to-end pipeline to ingest **1-minute OHLCV equity data** into a **ClickHouse** database.  
This tool supports:

- Reading data from local CSV files, or  
- Pulling end-of-day (EOD) data from the AngelOne API  

It automatically handles data preprocessing, identifies missing data, and ensures efficient insertion into the database.

## ğŸ§ Features

- âœ… Ingest 1-minute OHLCV equity data  
- âœ… Supports local directory or AngelOne API as data source  
- âœ… Identifies last available timestamp in database and fetches only new required data  
- âœ… Fetches and identifies newly listed equity scripts from AngelOne API  
- âœ… Cleans and preprocesses data before ingestion  
- âœ… Efficient and scalable insertion into ClickHouse supporting chunked uploads for large datasets  

## ğŸ§° How It Works

### ğŸ—‚ï¸ Mode 1: Local CSV Ingestion

```bash
1. Reads OHLCV `.csv` files from a specified local folder.
2. Cleans and validates the data (timestamp formatting, sorting, type casting).
3. Inserts the preprocessed records into ClickHouse.
```

### ğŸŒ Mode 2: EOD API-Based Ingestion (AngelOne)

```bash
1. Fetches the latest list of tradeable equity instruments via the AngelOne API.
2. For each instrument:
   - Checks if it already exists in the ClickHouse database.
   - If it exists, fetches the latest available timestamp.
   - Downloads new 1-minute OHLCV EOD equity data from that point onward.
3. Cleans and preprocesses the data.
4. Inserts the new data into ClickHouse.
```

> âš ï¸ This tool does **not support live intraday ingestion**. Data should be pulled **after market close (EOD only).**

## âš™ï¸ How to Use

All configuration is handled through a `.env` file. There are no command-line arguments required.

To run the tool:

```bash
python main.py
```

The pipeline automatically reads settings from your `.env` file to determine the mode (`api` or `local`) and other configurations.

## ğŸ” .env Configuration

Create a `.env` file in your project root with the following keys:

```env
# --- Data Source ---
DATA_SOURCE_MODE=api
LOCAL_DATA_FOLDER=./data/csv/

# --- AngelOne API Credentials ---
ANGELONE_API=semityapi
ANGELONE_SECRET_KEY=your_secret_key
ANGEL_ONE_PASSWORD=your_password
ANGEL_ONE_TOKEN=your_token
ANGEL_ONE_USER_ID=your_user_id
ANGEL_ONE_PIN=1100
ANGEL_ONE_API_SCRIPT=your_script_name

# --- ClickHouse Configuration ---
CLICKHOUSE_HOST=localhost
CLICKHOUSE_USERNAME=default
CLICKHOUSE_PASSWORD=your_password
CLICKHOUSE_PORT=8123
CLICKHOUSE_DATABASE=stock_data
CLICKHOUSE_TABLE=stock_ohlcv
```

> ğŸ›‘ Never commit your `.env` file. Ensure itâ€™s listed in `.gitignore`.

## ğŸ§ª ClickHouse Table Schema

```sql
CREATE TABLE stock_ohlcv (
  ticker LowCardinality(String),
  timestamp DateTime,
  open Float32,
  high Float32,
  low Float32,
  close Float32,
  volume Int64
) ENGINE = MergeTree()
ORDER BY (ticker, timestamp);
```

## ğŸ“ Project Structure

```
src/
â”œâ”€â”€ downloader/
â”‚   â”œâ”€â”€ angelone_api_client.py
â”‚   â””â”€â”€ fetch_local_data.py
â”œâ”€â”€ preprocess/
â”‚   â””â”€â”€ preprocess.py
â”œâ”€â”€ ingestion/
â”‚   â”œâ”€â”€ clickhouse.py
â”‚   â””â”€â”€ ingest_single.py
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ logger.py
main.py
```

## âœ… Requirements

- Python 3.8+
- ClickHouse server running
- AngelOne SmartAPI account

Install dependencies:

```bash
pip install -r requirements.txt
```

## ğŸ° Contribution Guidelines

Feel free to fork the repo, submit issues or pull requests.  
Suggestions to improve ingestion speed, support new APIs, or refactor logic are welcome.

## ğŸ’» Built With

- Python  
- AngelOne SmartAPI  
- ClickHouse
